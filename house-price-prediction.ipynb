{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\nThis notebook was copied and edited from SERIGNE's Stacked Regressions : Top 4% on LeaderBoard https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard;\n\nI borrowed how to drop highly correlated variables from ERIK BRUIN's House prices: Lasso, XGBoost, and a detailed EDA https://www.kaggle.com/code/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda.\n\nI also added a ratio feature following Kaggle Learn Course Feature Engineering https://www.kaggle.com/learn/feature-engineering.\n\nFinally I blended models following NANASHI's #1 House Prices Solution [top 1%] https://www.kaggle.com/code/jesucristo/1-house-prices-solution-top-1#New-blending\n\nHappy Kaggling!","metadata":{}},{"cell_type":"code","source":"#import some necessary librairies\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n%matplotlib inline\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder\n# from category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor \nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:15.112690Z","iopub.execute_input":"2024-01-25T20:52:15.113163Z","iopub.status.idle":"2024-01-25T20:52:18.584980Z","shell.execute_reply.started":"2024-01-25T20:52:15.113137Z","shell.execute_reply":"2024-01-25T20:52:18.583813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1 : Load the data.","metadata":{}},{"cell_type":"code","source":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:18.586928Z","iopub.execute_input":"2024-01-25T20:52:18.587274Z","iopub.status.idle":"2024-01-25T20:52:18.658127Z","shell.execute_reply.started":"2024-01-25T20:52:18.587244Z","shell.execute_reply":"2024-01-25T20:52:18.657135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##display the first five rows of the train dataset.\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:18.659490Z","iopub.execute_input":"2024-01-25T20:52:18.659984Z","iopub.status.idle":"2024-01-25T20:52:18.690806Z","shell.execute_reply.started":"2024-01-25T20:52:18.659955Z","shell.execute_reply":"2024-01-25T20:52:18.689812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##display the first five rows of the test dataset.\ntest.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:18.692903Z","iopub.execute_input":"2024-01-25T20:52:18.693260Z","iopub.status.idle":"2024-01-25T20:52:18.710644Z","shell.execute_reply.started":"2024-01-25T20:52:18.693238Z","shell.execute_reply":"2024-01-25T20:52:18.709808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:18.711666Z","iopub.execute_input":"2024-01-25T20:52:18.711918Z","iopub.status.idle":"2024-01-25T20:52:18.726388Z","shell.execute_reply.started":"2024-01-25T20:52:18.711896Z","shell.execute_reply":"2024-01-25T20:52:18.725258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Remove the outliers.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:18.728016Z","iopub.execute_input":"2024-01-25T20:52:18.728386Z","iopub.status.idle":"2024-01-25T20:52:19.012891Z","shell.execute_reply.started":"2024-01-25T20:52:18.728359Z","shell.execute_reply":"2024-01-25T20:52:19.011487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:19.014122Z","iopub.execute_input":"2024-01-25T20:52:19.014374Z","iopub.status.idle":"2024-01-25T20:52:19.240394Z","shell.execute_reply.started":"2024-01-25T20:52:19.014353Z","shell.execute_reply":"2024-01-25T20:52:19.239223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3:Normalize the target variable.","metadata":{}},{"cell_type":"code","source":"sns.histplot(train['SalePrice'], kde=True, stat=\"density\", kde_kws=dict(cut=3), alpha=.4, edgecolor=(1, 1, 1, .4), bins=50);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Plot the PDF. \nxmin, xmax = plt.xlim() \nx = np.linspace(xmin, xmax, 100) \np = norm.pdf(x, mu, sigma) \n  \nplt.plot(x, p, 'k', linewidth=2) \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:19.241654Z","iopub.execute_input":"2024-01-25T20:52:19.241884Z","iopub.status.idle":"2024-01-25T20:52:19.968926Z","shell.execute_reply.started":"2024-01-25T20:52:19.241863Z","shell.execute_reply":"2024-01-25T20:52:19.968012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.histplot(train['SalePrice'] , kde=True, stat=\"density\", kde_kws=dict(cut=3), alpha=.4, edgecolor=(1, 1, 1, .4), bins=50);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Plot the PDF. \nxmin, xmax = plt.xlim() \nx = np.linspace(xmin, xmax, 100) \np = norm.pdf(x, mu, sigma) \n  \nplt.plot(x, p, 'k', linewidth=2) \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:19.969936Z","iopub.execute_input":"2024-01-25T20:52:19.970173Z","iopub.status.idle":"2024-01-25T20:52:20.644420Z","shell.execute_reply.started":"2024-01-25T20:52:19.970152Z","shell.execute_reply":"2024-01-25T20:52:20.643481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:20.647490Z","iopub.execute_input":"2024-01-25T20:52:20.647799Z","iopub.status.idle":"2024-01-25T20:52:20.670065Z","shell.execute_reply.started":"2024-01-25T20:52:20.647773Z","shell.execute_reply":"2024-01-25T20:52:20.669168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Fill in missing data.","metadata":{}},{"cell_type":"code","source":"# 34 coloums have missing data.\n\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(40)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:20.671313Z","iopub.execute_input":"2024-01-25T20:52:20.671809Z","iopub.status.idle":"2024-01-25T20:52:20.687538Z","shell.execute_reply.started":"2024-01-25T20:52:20.671782Z","shell.execute_reply":"2024-01-25T20:52:20.686318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='vertical')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:20.689098Z","iopub.execute_input":"2024-01-25T20:52:20.689428Z","iopub.status.idle":"2024-01-25T20:52:21.401527Z","shell.execute_reply.started":"2024-01-25T20:52:20.689400Z","shell.execute_reply":"2024-01-25T20:52:21.400772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputing missing values","metadata":{}},{"cell_type":"code","source":"# PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n\n# MiscFeature : data description says NA means \"no misc feature\"\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\n# Alley : data description says NA means \"no alley access\"\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\n# Fence : data description says NA means \"no fence\"\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\n# FireplaceQu : data description says NA means \"no fireplace\"\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n# LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , \n# we can fill in missing values by the median LotFrontage of the neighborhood.\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \n# GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage).\nfor col in ('GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# GarageYrBlt: replacing missing values with median.\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(all_data['GarageYrBlt'].median())\n    \n# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \n# MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n# Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# Functional : data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n# SaleType : Fill in again with most frequent which is \"WD\"\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# MSSubClass : Na most likely means No building class. We can replace missing values with None\n# all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.402633Z","iopub.execute_input":"2024-01-25T20:52:21.403283Z","iopub.status.idle":"2024-01-25T20:52:21.451102Z","shell.execute_reply.started":"2024-01-25T20:52:21.403257Z","shell.execute_reply":"2024-01-25T20:52:21.450010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.452294Z","iopub.execute_input":"2024-01-25T20:52:21.452569Z","iopub.status.idle":"2024-01-25T20:52:21.472152Z","shell.execute_reply.started":"2024-01-25T20:52:21.452545Z","shell.execute_reply":"2024-01-25T20:52:21.471138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transforming some numerical variables that are really categorical.","metadata":{}},{"cell_type":"code","source":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.473079Z","iopub.execute_input":"2024-01-25T20:52:21.473341Z","iopub.status.idle":"2024-01-25T20:52:21.485631Z","shell.execute_reply.started":"2024-01-25T20:52:21.473318Z","shell.execute_reply":"2024-01-25T20:52:21.483980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ordinal Encoding some categorical variables that may contain information in their ordering set.\n# How to map categorical data to category_encoders.OrdinalEncoder in python pandas dataframe\n# https://stackoverflow.com/questions/50092911/how-to-map-categorical-data-to-category-encoders-ordinalencoder-in-python-pandas\n# ordinal_mappings = {\n#    \"MSSubClass\": [np.nan, 20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190], \n#    \"Street\": [np.nan, 'Grvl', 'Pave'],\n#    \"Alley\": ['None', 'Grvl', 'Pave'],\n#    \"LotShape\": [np.nan, 'IR3', 'IR2', 'IR1', 'Reg'], \n#    \"LandSlope\": [np.nan, 'Sev', 'Mod', 'Gtl'],\n#    \"OverallCond\": [np.nan, '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n#    \"ExterQual\": [np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"ExterCond\":[np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n#    \"BsmtQual\": ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"BsmtCond\": ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"BsmtExposure\": ['None', 'No', 'Mn', 'Av', 'Gd'], \n#    \"BsmtFinType1\": ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], \n#    \"BsmtFinType2\": ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], \n#    \"HeatingQC\": [np.nan, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n#    \"CentralAir\": [np.nan, 'N', 'Y'],\n#    \"KitchenQual\": [np.nan, 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"Functional\": [np.nan, 'Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'], \n#    \"FireplaceQu\": ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"GarageFinish\": ['None', 'Unf', 'RFn', 'Fin'], \n#    \"GarageQual\": ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"GarageCond\": ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"PavedDrive\": [np.nan, 'N', 'P', 'Y'],\n#    \"PoolQC\": ['None', 'Fa', 'TA', 'Gd', 'Ex'], \n#    \"Fence\": ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n# }\n\n\n# transform to a suitable format for OrdinalEncoder\n# ce_ordinal_mappings = []\n# for col, unique_values in ordinal_mappings.items():\n#    local_mapping = {val:idx for idx, val in enumerate(unique_values)}\n#    ce_ordinal_mappings.append({\"col\":col, \"mapping\":local_mapping})\n\n# encoder = OrdinalEncoder(mapping=ce_ordinal_mappings, return_df=True)\n# encoder.fit_transform(all_data)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.487783Z","iopub.execute_input":"2024-01-25T20:52:21.488159Z","iopub.status.idle":"2024-01-25T20:52:21.496907Z","shell.execute_reply.started":"2024-01-25T20:52:21.488128Z","shell.execute_reply":"2024-01-25T20:52:21.495491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label Encoding some categorical variables that may contain information in their ordering set.","metadata":{}},{"cell_type":"code","source":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n    \n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))    ","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.498208Z","iopub.execute_input":"2024-01-25T20:52:21.498969Z","iopub.status.idle":"2024-01-25T20:52:21.566077Z","shell.execute_reply.started":"2024-01-25T20:52:21.498937Z","shell.execute_reply":"2024-01-25T20:52:21.564623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Drop highly correlated variables.","metadata":{}},{"cell_type":"markdown","source":"According to the data description, some features may actually point to the same quantities. For example, 'GarageYrBlt' and 'YearBuilt' may be the same, because when the house was built, the garage was included. We draw the correlation map, and select those pairs that have a correlation factor greater than 0.8. ","metadata":{}},{"cell_type":"code","source":"#Correlation map to see how features are correlated with SalePrice\n# choose cols that only contains number\nnumeric_cols = train.select_dtypes(include='number')\n\n# then use this new col to do the next step\ncorrmat = numeric_cols.corr()\n\nplt.subplots(figsize=(25,25))  #this step is very important, without it you will miss many figure.\nsns.heatmap(corrmat, vmax=0.9, square=True, annot=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:21.567211Z","iopub.execute_input":"2024-01-25T20:52:21.567500Z","iopub.status.idle":"2024-01-25T20:52:25.606635Z","shell.execute_reply.started":"2024-01-25T20:52:21.567474Z","shell.execute_reply":"2024-01-25T20:52:25.605612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They are 'GarageYrBlt' and 'YearBuilt', 'GarageArea' and 'GarageCar', 'TotalBsmtSF' and '1stFlrSF', and 'TotRmsAbvGrd' and 'GrLivArea'. We keep the one of higher correlation with 'SalePrice' and delete the other. ","metadata":{}},{"cell_type":"code","source":"all_data.drop(['GarageYrBlt', 'GarageArea', '1stFlrSF', 'TotRmsAbvGrd'], axis = 1, inplace = True)\nprint('Shape all_data: {}'.format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.608193Z","iopub.execute_input":"2024-01-25T20:52:25.608515Z","iopub.status.idle":"2024-01-25T20:52:25.618973Z","shell.execute_reply.started":"2024-01-25T20:52:25.608490Z","shell.execute_reply":"2024-01-25T20:52:25.617663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Add new features.","metadata":{}},{"cell_type":"code","source":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['GrLivArea']\n\n# Adding total number of bathrooms\nall_data['TotalBathrooms'] = all_data['FullBath'] + (all_data['HalfBath']*0.5) + all_data['BsmtFullBath'] + (all_data['BsmtHalfBath']*0.5)\n\n# Adding total outside sqfootage\nall_data['TotalOutsideSF'] = all_data['WoodDeckSF'] + all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['3SsnPorch'] + all_data['ScreenPorch']\n\n# Adding a ratio feature\nall_data['LivLotRatio'] = all_data['GrLivArea'] / all_data['LotArea']\n\n# The value of a home often depends on how it compares to typical homes in its neighborhood. \n# Create a feature `MedNhbdArea` that describes the *median* of `GrLivArea` grouped on `Neighborhood`.\nall_data[\"MedNhbdArea\"] = all_data.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.620443Z","iopub.execute_input":"2024-01-25T20:52:25.621369Z","iopub.status.idle":"2024-01-25T20:52:25.639816Z","shell.execute_reply.started":"2024-01-25T20:52:25.621341Z","shell.execute_reply":"2024-01-25T20:52:25.638991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Normalize all numerical variables.","metadata":{}},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.640881Z","iopub.execute_input":"2024-01-25T20:52:25.641978Z","iopub.status.idle":"2024-01-25T20:52:25.703218Z","shell.execute_reply.started":"2024-01-25T20:52:25.641942Z","shell.execute_reply":"2024-01-25T20:52:25.702056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Box Cox Transformation of (highly) skewed features https://onlinestatbook.com/2/transformations/box-cox.html","metadata":{}},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.704360Z","iopub.execute_input":"2024-01-25T20:52:25.704653Z","iopub.status.idle":"2024-01-25T20:52:25.730812Z","shell.execute_reply.started":"2024-01-25T20:52:25.704629Z","shell.execute_reply":"2024-01-25T20:52:25.729721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.732170Z","iopub.execute_input":"2024-01-25T20:52:25.732535Z","iopub.status.idle":"2024-01-25T20:52:25.761699Z","shell.execute_reply.started":"2024-01-25T20:52:25.732501Z","shell.execute_reply":"2024-01-25T20:52:25.760416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Blend models.","metadata":{}},{"cell_type":"code","source":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.763087Z","iopub.execute_input":"2024-01-25T20:52:25.763396Z","iopub.status.idle":"2024-01-25T20:52:25.768868Z","shell.execute_reply.started":"2024-01-25T20:52:25.763367Z","shell.execute_reply":"2024-01-25T20:52:25.767828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.770147Z","iopub.execute_input":"2024-01-25T20:52:25.770474Z","iopub.status.idle":"2024-01-25T20:52:25.780430Z","shell.execute_reply.started":"2024-01-25T20:52:25.770444Z","shell.execute_reply":"2024-01-25T20:52:25.779638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LASSO Regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n# Elastic Net Regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n# Kernel Ridge Regression\nKRR = make_pipeline(RobustScaler(), KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\n\n# SVR\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\n# XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n# LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7\n                              # min_data_in_leaf =6, min_sum_hessian_in_leaf = 11\n                             )","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.781448Z","iopub.execute_input":"2024-01-25T20:52:25.781779Z","iopub.status.idle":"2024-01-25T20:52:25.792954Z","shell.execute_reply.started":"2024-01-25T20:52:25.781734Z","shell.execute_reply":"2024-01-25T20:52:25.792152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:25.793800Z","iopub.execute_input":"2024-01-25T20:52:25.794247Z","iopub.status.idle":"2024-01-25T20:52:26.422102Z","shell.execute_reply.started":"2024-01-25T20:52:25.794222Z","shell.execute_reply":"2024-01-25T20:52:26.420795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:26.435886Z","iopub.execute_input":"2024-01-25T20:52:26.440937Z","iopub.status.idle":"2024-01-25T20:52:27.189469Z","shell.execute_reply.started":"2024-01-25T20:52:26.440892Z","shell.execute_reply":"2024-01-25T20:52:27.188739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:27.193380Z","iopub.execute_input":"2024-01-25T20:52:27.195515Z","iopub.status.idle":"2024-01-25T20:52:28.110367Z","shell.execute_reply.started":"2024-01-25T20:52:27.195483Z","shell.execute_reply":"2024-01-25T20:52:28.109679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:28.111663Z","iopub.execute_input":"2024-01-25T20:52:28.112663Z","iopub.status.idle":"2024-01-25T20:52:29.331006Z","shell.execute_reply.started":"2024-01-25T20:52:28.112630Z","shell.execute_reply":"2024-01-25T20:52:29.329872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:52:29.332142Z","iopub.execute_input":"2024-01-25T20:52:29.332407Z","iopub.status.idle":"2024-01-25T20:53:26.425030Z","shell.execute_reply.started":"2024-01-25T20:52:29.332383Z","shell.execute_reply":"2024-01-25T20:53:26.423888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:53:26.426557Z","iopub.execute_input":"2024-01-25T20:53:26.427625Z","iopub.status.idle":"2024-01-25T20:53:35.582606Z","shell.execute_reply.started":"2024-01-25T20:53:26.427578Z","shell.execute_reply":"2024-01-25T20:53:35.581706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:53:35.583837Z","iopub.execute_input":"2024-01-25T20:53:35.585133Z","iopub.status.idle":"2024-01-25T20:54:17.065188Z","shell.execute_reply.started":"2024-01-25T20:53:35.585098Z","shell.execute_reply":"2024-01-25T20:54:17.063996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stacking models","metadata":{}},{"cell_type":"code","source":"# Stacking Ensemble Machine Learning With Python, https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n\nlevel0 = list()\nlevel0.append(('lasso', lasso))\nlevel0.append(('enet', ENet))\nlevel0.append(('krr', KRR))\nlevel0.append(('gboost', GBoost))\nlevel0.append(('xgb', model_xgb))\nlevel0.append(('lgbm', model_lgb))\n# define meta learner model\nlevel1 = model_xgb\nmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n\nscore = rmsle_cv(model)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T20:54:17.066520Z","iopub.execute_input":"2024-01-25T20:54:17.066870Z","iopub.status.idle":"2024-01-25T21:05:20.346797Z","shell.execute_reply.started":"2024-01-25T20:54:17.066841Z","shell.execute_reply":"2024-01-25T21:05:20.345929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensembling all the models defined previously.","metadata":{}},{"cell_type":"code","source":"# We first define a rmsle evaluation function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:20.348153Z","iopub.execute_input":"2024-01-25T21:05:20.348662Z","iopub.status.idle":"2024-01-25T21:05:20.353227Z","shell.execute_reply.started":"2024-01-25T21:05:20.348631Z","shell.execute_reply":"2024-01-25T21:05:20.352439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lasso\nlasso.fit(train.values, y_train)\nlasso_train_pred = lasso.predict(train.values)\nlasso_pred = np.expm1(lasso.predict(test.values))\nprint(rmsle(y_train, lasso_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:20.354368Z","iopub.execute_input":"2024-01-25T21:05:20.354875Z","iopub.status.idle":"2024-01-25T21:05:20.523733Z","shell.execute_reply.started":"2024-01-25T21:05:20.354846Z","shell.execute_reply":"2024-01-25T21:05:20.523025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enet\nENet.fit(train.values, y_train)\nenet_train_pred = ENet.predict(train.values)\nenet_pred = np.expm1(ENet.predict(test.values))\nprint(rmsle(y_train, enet_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:20.527355Z","iopub.execute_input":"2024-01-25T21:05:20.529477Z","iopub.status.idle":"2024-01-25T21:05:20.713908Z","shell.execute_reply.started":"2024-01-25T21:05:20.529444Z","shell.execute_reply":"2024-01-25T21:05:20.713225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# krr\nKRR.fit(train.values, y_train)\nkrr_train_pred = KRR.predict(train.values)\nkrr_pred = np.expm1(KRR.predict(test.values))\nprint(rmsle(y_train, krr_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:20.717696Z","iopub.execute_input":"2024-01-25T21:05:20.719692Z","iopub.status.idle":"2024-01-25T21:05:21.042766Z","shell.execute_reply.started":"2024-01-25T21:05:20.719663Z","shell.execute_reply":"2024-01-25T21:05:21.041840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# svr\nsvr.fit(train.values, y_train)\nsvr_train_pred = svr.predict(train.values)\nsvr_pred = np.expm1(svr.predict(test.values))\nprint(rmsle(y_train, svr_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:21.054516Z","iopub.execute_input":"2024-01-25T21:05:21.057288Z","iopub.status.idle":"2024-01-25T21:05:22.249667Z","shell.execute_reply.started":"2024-01-25T21:05:21.057253Z","shell.execute_reply":"2024-01-25T21:05:22.248828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gboost\nGBoost.fit(train.values, y_train)\ngboost_train_pred = GBoost.predict(train.values)\ngboost_pred = np.expm1(GBoost.predict(test.values))\nprint(rmsle(y_train, gboost_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:22.251063Z","iopub.execute_input":"2024-01-25T21:05:22.251629Z","iopub.status.idle":"2024-01-25T21:05:34.757661Z","shell.execute_reply.started":"2024-01-25T21:05:22.251599Z","shell.execute_reply":"2024-01-25T21:05:34.756828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scikit-learn StackingRegressor\nmodel.fit(train.values, y_train)\nstacking_train_pred = model.predict(train.values)\nstacking_pred = np.expm1(model.predict(test.values))\nprint(rmsle(y_train, stacking_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:05:34.759105Z","iopub.execute_input":"2024-01-25T21:05:34.759694Z","iopub.status.idle":"2024-01-25T21:07:56.028095Z","shell.execute_reply.started":"2024-01-25T21:05:34.759661Z","shell.execute_reply":"2024-01-25T21:07:56.026846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:07:56.029499Z","iopub.execute_input":"2024-01-25T21:07:56.029812Z","iopub.status.idle":"2024-01-25T21:08:01.148899Z","shell.execute_reply.started":"2024-01-25T21:07:56.029789Z","shell.execute_reply":"2024-01-25T21:08:01.147473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:08:01.150163Z","iopub.execute_input":"2024-01-25T21:08:01.150414Z","iopub.status.idle":"2024-01-25T21:08:09.460689Z","shell.execute_reply.started":"2024-01-25T21:08:01.150392Z","shell.execute_reply":"2024-01-25T21:08:09.459800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train, lasso_train_pred * 0.1 + \\\n                     enet_train_pred * 0.1 + \\\n                     krr_train_pred * 0.1 + \\\n                     svr_train_pred * 0.1 + \\\n                     gboost_train_pred * 0.1 + \\\n                     stacking_train_pred * 0.25 + \\\n                     xgb_train_pred * 0.15 + \\\n                     lgb_train_pred * 0.1 ))","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:08:09.462016Z","iopub.execute_input":"2024-01-25T21:08:09.462299Z","iopub.status.idle":"2024-01-25T21:08:09.468559Z","shell.execute_reply.started":"2024-01-25T21:08:09.462274Z","shell.execute_reply":"2024-01-25T21:08:09.467940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensemble prediction\nensemble = lasso_pred * 0.1 + \\\n           enet_pred * 0.1 + \\\n           krr_pred * 0.1 + \\\n           svr_pred * 0.1 + \\\n           gboost_pred * 0.1 + \\\n           stacking_pred * 0.25 + \\\n           xgb_pred * 0.15 + \\\n           lgb_pred * 0.1","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:08:09.469681Z","iopub.execute_input":"2024-01-25T21:08:09.470166Z","iopub.status.idle":"2024-01-25T21:08:09.484114Z","shell.execute_reply.started":"2024-01-25T21:08:09.470134Z","shell.execute_reply":"2024-01-25T21:08:09.483355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 9: Submission.","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('/kaggle/working/submission.csv',index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T21:08:09.485324Z","iopub.execute_input":"2024-01-25T21:08:09.485889Z","iopub.status.idle":"2024-01-25T21:08:09.510052Z","shell.execute_reply.started":"2024-01-25T21:08:09.485849Z","shell.execute_reply":"2024-01-25T21:08:09.509184Z"},"trusted":true},"execution_count":null,"outputs":[]}]}