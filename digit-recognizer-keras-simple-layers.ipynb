{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I constructed this notebook following Kaggle Learn Intro to Deep Learning https://www.kaggle.com/learn/intro-to-deep-learning and TensorFlow tutorials https://www.tensorflow.org/tutorials/quickstart/beginner. I have introduced dropout to correct overfitting, and used early stopping to prevent both underfitting and overfitting. Other main references include: NURSULTAN KURMANBEKOV's digit-recognizer-mnist https://www.kaggle.com/code/nursultankurmanbekov/digit-recognizer-mnist and YASSINE GHOUZAM's Introduction to CNN Keras - 0.997 (top 6%) https://www.kaggle.com/code/yassineghouzam/introduction-to-cnn-keras-0-997-top-6.\n\nAs pointed in Chapter 2 of Deep Learning with Python by Fran√ßois Chollet, this simple model suffers from overfitting. So I switched to convnets approach in a separate notebook https://www.kaggle.com/code/garfield2021/digit-recognizer-keras-cnn. The test accuracy score increased from ~0.97 to ~0.989. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:47.712129Z","iopub.execute_input":"2024-04-03T19:45:47.712847Z","iopub.status.idle":"2024-04-03T19:45:52.191490Z","shell.execute_reply.started":"2024-04-03T19:45:47.712802Z","shell.execute_reply":"2024-04-03T19:45:52.190457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Process the Data","metadata":{}},{"cell_type":"code","source":"# Load the train and test datasets.\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:52.193250Z","iopub.execute_input":"2024-04-03T19:45:52.193855Z","iopub.status.idle":"2024-04-03T19:45:56.902252Z","shell.execute_reply.started":"2024-04-03T19:45:52.193827Z","shell.execute_reply":"2024-04-03T19:45:56.901398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = train[\"label\"]\nX_train = train.drop(labels=[\"label\"], axis=1)\n\ndel train","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:56.903422Z","iopub.execute_input":"2024-04-03T19:45:56.903727Z","iopub.status.idle":"2024-04-03T19:45:56.984436Z","shell.execute_reply.started":"2024-04-03T19:45:56.903702Z","shell.execute_reply":"2024-04-03T19:45:56.983625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The count distribution for 10 digits are close to uniform.\ng = sns.histplot(data=Y_train)\nY_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:56.985586Z","iopub.execute_input":"2024-04-03T19:45:56.985911Z","iopub.status.idle":"2024-04-03T19:45:57.387943Z","shell.execute_reply.started":"2024-04-03T19:45:56.985884Z","shell.execute_reply":"2024-04-03T19:45:57.386786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No missing values.\nX_train.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.390411Z","iopub.execute_input":"2024-04-03T19:45:57.390723Z","iopub.status.idle":"2024-04-03T19:45:57.425528Z","shell.execute_reply.started":"2024-04-03T19:45:57.390698Z","shell.execute_reply":"2024-04-03T19:45:57.424515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.426844Z","iopub.execute_input":"2024-04-03T19:45:57.427674Z","iopub.status.idle":"2024-04-03T19:45:57.450437Z","shell.execute_reply.started":"2024-04-03T19:45:57.427633Z","shell.execute_reply":"2024-04-03T19:45:57.449469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"test shape: \", test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.451684Z","iopub.execute_input":"2024-04-03T19:45:57.452079Z","iopub.status.idle":"2024-04-03T19:45:57.458047Z","shell.execute_reply.started":"2024-04-03T19:45:57.452045Z","shell.execute_reply":"2024-04-03T19:45:57.457169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Normalize and Split","metadata":{}},{"cell_type":"markdown","source":"The pixel values of the images range from 0 through 255. Scale these values to a range of 0 to 1 by dividing the values by 255.0. This also converts the sample data from integers to floating-point numbers.","metadata":{}},{"cell_type":"code","source":"X_train, test = X_train / 255.0, test / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.459314Z","iopub.execute_input":"2024-04-03T19:45:57.459694Z","iopub.status.idle":"2024-04-03T19:45:57.555639Z","shell.execute_reply.started":"2024-04-03T19:45:57.459664Z","shell.execute_reply":"2024-04-03T19:45:57.554548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = to_categorical(Y_train, num_classes = 10)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.557034Z","iopub.execute_input":"2024-04-03T19:45:57.557433Z","iopub.status.idle":"2024-04-03T19:45:57.564178Z","shell.execute_reply.started":"2024-04-03T19:45:57.557394Z","shell.execute_reply":"2024-04-03T19:45:57.563220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stratify - make sure classes are evenlly represented across splits\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, stratify=Y_train, train_size=0.75)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:57.565282Z","iopub.execute_input":"2024-04-03T19:45:57.565592Z","iopub.status.idle":"2024-04-03T19:45:58.416395Z","shell.execute_reply.started":"2024-04-03T19:45:57.565567Z","shell.execute_reply":"2024-04-03T19:45:58.415405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X_train.values.reshape(-1,28,28,1)[0], cmap=plt.get_cmap('gray'))","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:58.417651Z","iopub.execute_input":"2024-04-03T19:45:58.417991Z","iopub.status.idle":"2024-04-03T19:45:58.686796Z","shell.execute_reply.started":"2024-04-03T19:45:58.417963Z","shell.execute_reply":"2024-04-03T19:45:58.685723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_val shape: \", X_val.shape)\nprint(\"Y_val shape: \", Y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:58.688277Z","iopub.execute_input":"2024-04-03T19:45:58.688810Z","iopub.status.idle":"2024-04-03T19:45:58.694203Z","shell.execute_reply.started":"2024-04-03T19:45:58.688775Z","shell.execute_reply":"2024-04-03T19:45:58.693343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Build the Model","metadata":{}},{"cell_type":"markdown","source":"I have chosen a three-layer network with over 500 neurons. This network should be capable of learning the relationships in the data. The Sequential model will connect together a list of layers in order from first to last: \nthe first layer gets the input, the last layer produces the output. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n        \n    # the hidden ReLU layers\n    Dense(256, activation='relu'),\n    # apply 20% dropout to the next layer\n    Dropout(0.2),\n    \n    Dense(256, activation='relu'),\n    Dropout(0.2),\n    \n    # the multi-class output layers\n    Dense(10, activation='softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:58.695359Z","iopub.execute_input":"2024-04-03T19:45:58.695724Z","iopub.status.idle":"2024-04-03T19:45:59.273417Z","shell.execute_reply.started":"2024-04-03T19:45:58.695695Z","shell.execute_reply":"2024-04-03T19:45:59.272660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After defining the model, we compile in the optimizer and loss function. \nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:59.276756Z","iopub.execute_input":"2024-04-03T19:45:59.277031Z","iopub.status.idle":"2024-04-03T19:45:59.294133Z","shell.execute_reply.started":"2024-04-03T19:45:59.277007Z","shell.execute_reply":"2024-04-03T19:45:59.293259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore, i.e., early stopping. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough.","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    \n    # how many epochs to wait before stopping\n    patience=5,\n    \n    # minimium amount of change to count as an improvement\n    min_delta=0.001,\n    \n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:59.295215Z","iopub.execute_input":"2024-04-03T19:45:59.295514Z","iopub.status.idle":"2024-04-03T19:45:59.299846Z","shell.execute_reply.started":"2024-04-03T19:45:59.295469Z","shell.execute_reply":"2024-04-03T19:45:59.298974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've told Keras to feed the optimizer 512 rows of the training data at a time (the batch_size) and to do that 100 times all the way through the dataset (the epochs). You can see that Keras will keep you updated on the loss as the model trains.","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    X_train, Y_train,\n    validation_data=(X_val, Y_val),\n    batch_size=128,\n    epochs=10,\n    \n    # put your callbacks in a list\n    callbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:45:59.300848Z","iopub.execute_input":"2024-04-03T19:45:59.301096Z","iopub.status.idle":"2024-04-03T19:46:15.876148Z","shell.execute_reply.started":"2024-04-03T19:45:59.301074Z","shell.execute_reply":"2024-04-03T19:46:15.874826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Often, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We'll convert the data to a Pandas dataframe, which makes the plotting easy. The model overfits the training data at best accuracy close to 0.99 but underfits the validation set at accuracy smaller than 0.98.","metadata":{}},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\nprint(\"Maximum validation accuracy: {}\".format(history_df['val_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:46:15.877828Z","iopub.execute_input":"2024-04-03T19:46:15.878162Z","iopub.status.idle":"2024-04-03T19:46:16.527010Z","shell.execute_reply.started":"2024-04-03T19:46:15.878135Z","shell.execute_reply":"2024-04-03T19:46:16.525830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Predict and Submit","metadata":{}},{"cell_type":"code","source":"# Make predicitons based on the model trained before.\npredictions = model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:46:16.528290Z","iopub.execute_input":"2024-04-03T19:46:16.528631Z","iopub.status.idle":"2024-04-03T19:46:19.022619Z","shell.execute_reply.started":"2024-04-03T19:46:16.528602Z","shell.execute_reply":"2024-04-03T19:46:19.021555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the index with the maximum probability\npredictions = np.argmax(predictions,axis =1)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:46:19.024138Z","iopub.execute_input":"2024-04-03T19:46:19.024467Z","iopub.status.idle":"2024-04-03T19:46:19.030375Z","shell.execute_reply.started":"2024-04-03T19:46:19.024439Z","shell.execute_reply":"2024-04-03T19:46:19.029086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.Series(predictions, name='Label')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:46:19.031856Z","iopub.execute_input":"2024-04-03T19:46:19.032219Z","iopub.status.idle":"2024-04-03T19:46:19.043718Z","shell.execute_reply.started":"2024-04-03T19:46:19.032179Z","shell.execute_reply":"2024-04-03T19:46:19.042783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), predictions],axis = 1)\nsubmission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T19:46:19.044964Z","iopub.execute_input":"2024-04-03T19:46:19.045275Z","iopub.status.idle":"2024-04-03T19:46:19.096197Z","shell.execute_reply.started":"2024-04-03T19:46:19.045251Z","shell.execute_reply":"2024-04-03T19:46:19.095260Z"},"trusted":true},"execution_count":null,"outputs":[]}]}